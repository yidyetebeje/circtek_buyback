import { migrateOldData, migrateOldDataBatch } from './migrate-old-data'
import * as fs from 'fs'
import * as path from 'path'

/**
 * Migration script that fetches data from the old API and migrates it
 * 
 * Run with: bun run src/diagnostics/run-migration-from-live-api.ts
 * 
 * This script:
 * 1. Fetches all data from the old API (paginated)
 * 2. Appends each page of data to response.json as it's fetched
 * 3. Transforms and migrates it to the new system
 * 4. Tracks progress and errors
 * 
 * Note: Data is saved incrementally to response.json to avoid memory issues
 * with large datasets and to provide progress persistence.
 */

// Configuration
const API_CONFIG = {
	baseUrl: 'http://localhost:3000/v1/user/getCloudDbDevices',
	clientId: 0,
	pageSize: 100, // Fetch 500 records per page
	searchText: '',
	searchType: 'imei',
}

// Migration Configuration
const MIGRATION_CONFIG = {
	testerId: 9,    // Update if needed
	tenantId: 10,    // Update if needed
	warehouseId: 1, // Update if needed
}

interface ApiResponse {
	status: boolean
	message: string
	data: {
		data: any[]
		totalPages: number
		totalItems: number
		currentPage: number
	}
}

/**
 * Initialize the response.json file with an empty array
 */
function initializeResponseFile(): void {
	const filePath = 'response.json'
	try {
		fs.writeFileSync(filePath, '[]')
		console.log('üìÑ Initialized response.json file')
	} catch (error) {
		console.error('‚ùå Failed to initialize response.json:', error)
	}
}

/**
 * Append data to the response.json file
 */
function appendToResponseFile(data: any[]): void {
	const filePath = 'response.json'
	try {
		// Read existing data
		let existingData: any[] = []
		if (fs.existsSync(filePath)) {
			const fileContent = fs.readFileSync(filePath, 'utf8')
			existingData = JSON.parse(fileContent)
		}
		
		// Append new data
		existingData.push(...data)
		
		// Write back to file
		fs.writeFileSync(filePath, JSON.stringify(existingData, null, 2))
		console.log(`üíæ Appended ${data.length} records to response.json (Total: ${existingData.length})`)
	} catch (error) {
		console.error('‚ùå Failed to append to response.json:', error)
	}
}

/**
 * Fetch a single page of data from the API
 */
async function fetchPage(page: number): Promise<ApiResponse | null> {
	const url = new URL(API_CONFIG.baseUrl)
	url.searchParams.set('clientId', API_CONFIG.clientId.toString())
	url.searchParams.set('page', page.toString())
	url.searchParams.set('size', API_CONFIG.pageSize.toString())
	url.searchParams.set('searchText', API_CONFIG.searchText)
	url.searchParams.set('searchType', API_CONFIG.searchType)
	

	try {
		console.log(`üì• Fetching page ${page}...`)
		const response = await fetch(url.toString(), {
			headers: {
				"Authorization": "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6MjgsImZOYW1lIjoiWWlkbmVrYWNoZXciLCJsTmFtZSI6IlRlYmVqZSIsIndhcmVob3VzZUlkIjowLCJjbGllbnRJZCI6MCwidXNlck5hbWUiOiJZaWRuZWthY2hldyIsImVtYWlsIjoieWlkbmVrYWNoZXd0ZWJlamVAZ21haWwuY29tIiwicm9sZUlkIjoxLCJyb2xlTmFtZSI6IkFkbWluIiwicm9sZVNsdWciOiJhZG1pbiIsImlhdCI6MTc1OTQ4MzU2OSwiZXhwIjoxNzY4MTIzNTY5fQ.0fNaq1wLEXj0ABHUyvMuUhcOJZbwOcqtoNx4THeeQnA",
				"token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6MjgsImZOYW1lIjoiWWlkbmVrYWNoZXciLCJsTmFtZSI6IlRlYmVqZSIsIndhcmVob3VzZUlkIjowLCJjbGllbnRJZCI6MCwidXNlck5hbWUiOiJZaWRuZWthY2hldyIsImVtYWlsIjoieWlkbmVrYWNoZXd0ZWJlamVAZ21haWwuY29tIiwicm9sZUlkIjoxLCJyb2xlTmFtZSI6IkFkbWluIiwicm9sZVNsdWciOiJhZG1pbiIsImlhdCI6MTc1OTQ5MTM1MSwiZXhwIjoxNzY4MTMxMzUxfQ.EvVRJ2Vo7U2gYTS0c2-ha1Iub-KEvMiIwTsL_C47ZzE"
			}
		})
		console.log(response, "response")
		
		if (!response.ok) {
			console.error(`‚ùå HTTP Error: ${response.status} ${response.statusText}`)
			return null
		}

		const data: ApiResponse = await response.json()
		
		if (!data.status) {
			console.error(`‚ùå API Error: ${data.message}`)
			return null
		}

		return data
	} catch (error) {
		console.error(`‚ùå Fetch Error:`, error)
		return null
	}
}

/**
 * Fetch all pages from the API and append to response.json
 */
async function fetchAllData(): Promise<number> {
	let currentPage = 0
	let totalPages = 1
	let totalRecords = 0

	console.log('üåê Starting data fetch from API...')
	console.log(`üìç Endpoint: ${API_CONFIG.baseUrl}`)
	console.log(`üì¶ Page Size: ${API_CONFIG.pageSize}\n`)

	// Initialize the response file
	initializeResponseFile()

	while (currentPage < totalPages) {
		const response = await fetchPage(currentPage)
		if (!response) {
			console.error(`‚ùå Failed to fetch page ${currentPage}. Stopping.`)
			break
		}
		const pageData = response.data.data || []
		totalPages = response.data.totalPages
		totalRecords += pageData.length
		
		// Append data to file immediately
		appendToResponseFile(pageData)
		
		const totalItems = response.data.totalItems
		console.log(`‚úÖ Page ${currentPage + 1}/${totalPages} - Fetched ${pageData.length} records (Total so far: ${totalRecords}/${totalItems})`)
		currentPage++
		
		// Add a small delay to avoid overwhelming the API
		if (currentPage < totalPages) {
			await new Promise(resolve => setTimeout(resolve, 1000))
		}
	}
	console.log(`\n‚úÖ Fetch complete! Total records: ${totalRecords}\n`)
	return totalRecords
}

/**
 * Main migration function
 */
async function runMigrationFromLiveAPI() {
	console.log('=' .repeat(60))
	console.log('üöÄ MIGRATION FROM LIVE API')
	console.log('='.repeat(60))
	console.log()

	const startTime = Date.now()

	// Step 1: Fetch all data from API and save to response.json
	const totalRecords = await fetchAllData()

	if (totalRecords === 0) {
		console.error('‚ùå No data fetched from API. Exiting.')
		process.exit(1)
	}

	// Read the data from response.json for migration
	let oldData: any[] = []
	try {
		const fileContent = fs.readFileSync('response.json', 'utf8')
		oldData = JSON.parse(fileContent)
		console.log(`üìñ Loaded ${oldData.length} records from response.json`)
	} catch (error) {
		console.error('‚ùå Failed to read response.json:', error)
		process.exit(1)
	}

	
	const firstRecord = oldData[0]
	const config = {
		testerId: firstRecord.testerId || MIGRATION_CONFIG.testerId,
		tenantId: firstRecord.clientId || MIGRATION_CONFIG.tenantId,
		warehouseId: firstRecord.warehouseId || MIGRATION_CONFIG.warehouseId,
	}

	console.log('üìã Migration Configuration:')
	console.log(`   Tester ID: ${config.testerId}`)
	console.log(`   Tenant ID: ${config.tenantId}`)
	console.log(`   Warehouse ID: ${config.warehouseId}`)
	console.log()

	// Step 3: Migrate data
	console.log('üîÑ Starting migration process...')
	console.log(`üìä Total records to migrate: ${oldData.length}`)
	console.log('‚è≥ This may take a while...\n')

	// Choose migration method based on data size
	const useBatch = oldData.length > 50
	console.log(`üì¶ Using ${useBatch ? 'batch' : 'detailed'} migration method\n`)

	const migrationStartTime = Date.now()

	const result = useBatch
		? await migrateOldDataBatch(oldData, config)
		: await migrateOldData(oldData, config)

	const migrationDuration = ((Date.now() - migrationStartTime) / 1000).toFixed(2)
	const totalDuration = ((Date.now() - startTime) / 1000).toFixed(2)

	// Step 4: Display results
	console.log('\n' + '='.repeat(60))
	console.log('üìà MIGRATION COMPLETE')
	console.log('='.repeat(60))
	console.log(`‚úÖ Success: ${result.success}`)
	console.log(`‚ùå Failed: ${result.failed}`)
	console.log(`üìä Success Rate: ${((result.success / oldData.length) * 100).toFixed(1)}%`)
	console.log(`‚è±Ô∏è  Fetch Duration: ${((migrationStartTime - startTime) / 1000).toFixed(2)}s`)
	console.log(`‚è±Ô∏è  Migration Duration: ${migrationDuration}s`)
	console.log(`‚è±Ô∏è  Total Duration: ${totalDuration}s`)

	if (result.errors.length > 0) {
		console.log('\n‚ùå Errors:')
		result.errors.slice(0, 10).forEach((err, idx) => {
			const identifier = err.record.imei || err.record.serial || err.record.lpn || 'unknown'
			console.log(`  ${idx + 1}. ${identifier}: ${err.error}`)
		})
		
		if (result.errors.length > 10) {
			console.log(`  ... and ${result.errors.length - 10} more errors`)
		}

		// Save errors to file
		try {
			await Bun.write(
				'migration-errors.json',
				JSON.stringify(result.errors, null, 2)
			)
			console.log('\nüíæ All errors saved to: migration-errors.json')
		} catch (error) {
			console.error('Failed to save errors to file:', error)
		}
	}

	console.log('\n‚ú® Migration process finished!')
	console.log('='.repeat(60))
}

// Run the migration
runMigrationFromLiveAPI().catch((error) => {
	console.error('\nüí• Fatal error during migration:')
	console.error(error)
	process.exit(1)
})
